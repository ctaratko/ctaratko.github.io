[
  {
    "objectID": "danl200-hw5-Taratko-Christopher.html#word",
    "href": "danl200-hw5-Taratko-Christopher.html#word",
    "title": "danl200-hw5-Taratko-Christopher.qmd",
    "section": "Word",
    "text": "Word\n\n2 * 2\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "DANL Project",
    "section": "",
    "text": "The goal intended of the research and analysis of the stocks performed was to help people understand what certain stocks would classify as, whether it be a good investment overall, something preferable for the short/long run, what the volatility of a stock is, and other performance functions that would help the reader grasp what to look for when investing."
  },
  {
    "objectID": "project.html#summary-statistics",
    "href": "project.html#summary-statistics",
    "title": "DANL Project",
    "section": "2.1 Summary Statistics",
    "text": "2.1 Summary Statistics\n\n\nCode\nsummary(StocksUpdated2)\n\n\n   Company              Date             Close/Last         Volume         \n Length:25160       Length:25160       Min.   :  1.62   Min.   :1.144e+06  \n Class :character   Class :character   1st Qu.: 36.57   1st Qu.:1.200e+07  \n Mode  :character   Mode  :character   Median : 65.68   Median :2.672e+07  \n                                       Mean   :102.46   Mean   :5.132e+07  \n                                       3rd Qu.:134.24   3rd Qu.:6.857e+07  \n                                       Max.   :691.69   Max.   :1.065e+09  \n      Open             High             Low        \n Min.   :  1.62   Min.   :  1.69   Min.   :  1.61  \n 1st Qu.: 36.51   1st Qu.: 36.89   1st Qu.: 36.13  \n Median : 65.65   Median : 66.48   Median : 64.92  \n Mean   :102.43   Mean   :103.83   Mean   :101.01  \n 3rd Qu.:134.32   3rd Qu.:136.23   3rd Qu.:132.66  \n Max.   :692.35   Max.   :700.99   Max.   :686.09"
  },
  {
    "objectID": "project.html#historical-volatility-of-each-company-for-each-year.",
    "href": "project.html#historical-volatility-of-each-company-for-each-year.",
    "title": "DANL Project",
    "section": "3.1 Historical volatility of each company for each year.",
    "text": "3.1 Historical volatility of each company for each year.\nA simple measure of historical volatility is calculated by finding the standard deviation of the close prices over a time series. In this case we found the standard deviation for each stock for each year and then showed this data on ggplot.\n\n\nCode\nHistorical_Volatility &lt;- Volatility_analysis %&gt;% \n  group_by(Company, Year) %&gt;% \n  summarise(Volatility = sd(`Close/Last`))\n\nVolatility &lt;- ggplot(Historical_Volatility,\n       mapping = aes(x = as.numeric(Year),\n                     y = Volatility\n                     )) +\n  geom_point(color = \"blue\") +\n  geom_line(color = \"green\") +\n  facet_wrap( . ~Company) +\n  ggtitle(\"Stock Price Volatility from 2013-2023\", subtitle = \"In dollars\") +\n  labs(x = \"Year\") +\n  theme_economist_white()\n\nVolatility + scale_x_continuous(breaks = scales::breaks_width(2))"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html",
    "title": "- Part 1 -",
    "section": "",
    "text": "import pandas as pd\nnyc_payroll = pd.read_csv('https://bcdanl.github.io/data/nyc_payroll_2024.csv')\nnyc_payroll\n\n\n  \n    \n\n\n\n\n\n\nFiscal_Year\nPayroll_Number\nAgency_Name\nLast_Name\nFirst_Name\nMid_Init\nAgency_Start_Date\nWork_Location_Borough\nTitle_Description\nLeave_Status_as_of_June_30\nBase_Salary\nPay_Basis\nRegular_Hours\nRegular_Gross_Paid\nOT_Hours\nTotal_OT_Paid\nTotal_Other_Pay\n\n\n\n\n0\n2019\n742.0\nDEPT OF ED PEDAGOGICAL\nSLOMA\nJENNIFER\nNaN\n09/03/2002\nMANHATTAN\nTEACHER\nACTIVE\n104050.00\nper Annum\n0.00\n110768.35\n0.00\n0.0\n0.00\n\n\n1\n2020\n745.0\nDEPT OF ED HRLY SUPPORT STAFF\nGARFO\nMAWUFEMOR\nNaN\n10/01/2018\nMANHATTAN\nSTUDENT AIDE\nCEASED\n15.00\nper Hour\n0.00\n1552.50\n0.00\n0.0\n0.00\n\n\n2\n2018\n300.0\nBOARD OF ELECTION POLL WORKERS\nLAGRANDIER\nROSE\nNaN\n01/01/2015\nMANHATTAN\nELECTION WORKER\nACTIVE\n1.00\nper Hour\n0.00\n300.00\n0.00\n0.0\n0.00\n\n\n3\n2015\nNaN\nFIRE DEPARTMENT\nBRENGEL\nJERARD\nT\n01/27/2002\nQUEENS\nFIREFIGHTER\nACTIVE\n76488.00\nper Annum\n2085.72\n76278.53\n551.98\n34608.8\n6800.55\n\n\n4\n2018\n300.0\nBOARD OF ELECTION POLL WORKERS\nCOOPER\nJIMMIE\nNaN\n01/01/2013\nMANHATTAN\nELECTION WORKER\nACTIVE\n1.00\nper Hour\n0.00\n575.00\n0.00\n0.0\n0.00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n2021\n747.0\nDEPT OF ED PER SESSION TEACHER\nLO MEDICO\nLUCIA\nF\n08/29/2008\nMANHATTAN\nTEACHER- PER SESSION\nACTIVE\n33.18\nper Day\n0.00\n576.51\n0.00\n0.0\n11.01\n\n\n4996\n2015\nNaN\nDEPT OF ED PER SESSION TEACHER\nBLAKE\nLARON\nR\n12/28/2010\nMANHATTAN\nTEACHER- PER SESSION\nACTIVE\n33.18\nper Day\n0.00\n13731.65\n0.00\n0.0\n0.30\n\n\n4997\n2023\n742.0\nDEPT OF ED PEDAGOGICAL\nMCKENZIE\nCHERYL\nE\n09/04/2001\nMANHATTAN\nTEACHER ASSIGNED A\nACTIVE\n122424.00\nper Annum\n0.00\n122424.00\n0.00\n0.0\n0.00\n\n\n4998\n2018\n742.0\nDEPT OF ED PEDAGOGICAL\nNORRIS\nCATHLIN\nNaN\n09/02/2014\nMANHATTAN\nTEACHER SPECIAL EDUCATION\nACTIVE\n73238.00\nper Annum\n0.00\n70801.62\n0.00\n0.0\n0.00\n\n\n4999\n2022\n904.0\nDISTRICT ATTORNEY QNS COUNTY\nBYER\nPAMELA\nNaN\n08/06/1990\nQUEENS\nASSISTANT DISTRICT ATTORNEY\nACTIVE\n165000.00\nper Annum\n1820.00\n160819.04\n0.00\n0.0\n3923.00\n\n\n\n\n5000 rows × 17 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-1",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-1",
    "title": "- Part 1 -",
    "section": "Question #1:",
    "text": "Question #1:\nSelect “First_Name”, “Last_Name”, “Base_Salary”, and “Total_OT_Paid”, then sort the DataFrame with these selected variables by “Base_Salary” in descending order and display the top 10 entries.\n\nQ1 = (\n    nyc_payroll[[\"First_Name\", \"Last_Name\", \"Base_Salary\", \"Total_OT_Paid\"]]\n    .sort_values(\"Base_Salary\", ascending = False)\n    .head(10)\n)\nQ1\n\n\n  \n    \n\n\n\n\n\n\nFirst_Name\nLast_Name\nBase_Salary\nTotal_OT_Paid\n\n\n\n\n2090\nYVONNE\nMILEWSKI\n221268.0\n2820.39\n\n\n1825\nKENNETH\nGODINER\n221000.0\n0.00\n\n\n3964\nBRUCE\nJORDAN\n215861.0\n0.00\n\n\n1062\nMELINDA\nKATZ\n212800.0\n0.00\n\n\n277\nMELANIE\nHARTZOG\n207518.0\n0.00\n\n\n2580\nERIC\nBOORSTYN\n193968.0\n0.00\n\n\n370\nJOANNE\nRUSSELL\n191468.0\n0.00\n\n\n2007\nPATRICK\nKELLY\n185886.0\n0.00\n\n\n1412\nDEIRDRE\nSNYDER\n184250.0\n0.00\n\n\n2087\nOLGA\nMALUF\n183201.0\n0.00"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-2",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-2",
    "title": "- Part 1 -",
    "section": "Question #2:",
    "text": "Question #2:\nUsing set_index(), change the DataFrame’s index to “Last_Name”, then locate the data for a specific last name, say “BROWN”, and display their “Agency_Name”, “Base_Salary”, and “Total_OT_Paid”.\n\nQ2 = (\n    nyc_payroll[[\"Last_Name\",\"Agency_Name\", \"Base_Salary\", \"Total_OT_Paid\"]]\n    .set_index(\"Last_Name\")\n    .loc[\"BROWN\"]\n)\nQ2\n\n\n  \n    \n\n\n\n\n\n\nAgency_Name\nBase_Salary\nTotal_OT_Paid\n\n\nLast_Name\n\n\n\n\n\n\n\nBROWN\nPOLICE DEPARTMENT\n51574.00\n0.00\n\n\nBROWN\nDEPT OF HEALTH/MENTAL HYGIENE\n35538.00\n0.00\n\n\nBROWN\nDEPT OF ED PER SESSION TEACHER\n33.18\n0.00\n\n\nBROWN\nDEPT OF ED PEDAGOGICAL\n55670.00\n0.00\n\n\nBROWN\nDEPT. OF HOMELESS SERVICES\n39438.00\n0.00\n\n\nBROWN\nNYC HOUSING AUTHORITY\n40201.00\n471.35\n\n\nBROWN\nBOARD OF ELECTION POLL WORKERS\n1.00\n0.00\n\n\nBROWN\nDEPT OF HEALTH/MENTAL HYGIENE\n111288.00\n0.00\n\n\nBROWN\nDEPT OF ED HRLY SUPPORT STAFF\n12.66\n0.00\n\n\nBROWN\nDEPARTMENT OF SANITATION\n77318.00\n10669.03\n\n\nBROWN\nDEPT OF ED HRLY SUPPORT STAFF\n16.19\n0.00\n\n\nBROWN\nDEPARTMENT OF EDUCATION ADMIN\n68664.00\n0.00\n\n\nBROWN\nDEPT OF ED PEDAGOGICAL\n69977.00\n0.00\n\n\nBROWN\nDEPT OF ED PARA PROFESSIONALS\n26946.00\n1.76\n\n\nBROWN\nPOLICE DEPARTMENT\n125531.00\n35904.74\n\n\nBROWN\nDEPARTMENT OF CORRECTION\n92073.00\n3253.36\n\n\nBROWN\nDEPARTMENT OF EDUCATION ADMIN\n65663.00\n0.00"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-3",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-3",
    "title": "- Part 1 -",
    "section": "Question #3:",
    "text": "Question #3:\nFind the 5 employees with the highest “Regular_Gross_Paid” and calculate their average “OT_Hours”. Also, reset the index if you have changed it previously.\n\nQ3 = (\n    nyc_payroll[[\"First_Name\", \"Last_Name\",\"Regular_Gross_Paid\", \"OT_Hours\",\"Total_OT_Paid\"]]\n    .sort_values(\"Regular_Gross_Paid\", ascending = False)\n    .head(5)\n)\nQ3[\"Average_OT\"] = Q3[\"Total_OT_Paid\"] / Q3[\"OT_Hours\"]\nQ3\n\n\n  \n    \n\n\n\n\n\n\nFirst_Name\nLast_Name\nRegular_Gross_Paid\nOT_Hours\nTotal_OT_Paid\nAverage_OT\n\n\n\n\n2090\nYVONNE\nMILEWSKI\n220072.05\n25.0\n2820.39\n112.8156\n\n\n3964\nBRUCE\nJORDAN\n215253.44\n0.0\n0.00\nNaN\n\n\n277\nMELANIE\nHARTZOG\n213539.04\n0.0\n0.00\nNaN\n\n\n1062\nMELINDA\nKATZ\n212216.94\n0.0\n0.00\nNaN\n\n\n1825\nKENNETH\nGODINER\n211861.98\n0.0\n0.00\nNaN"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-4",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-4",
    "title": "- Part 1 -",
    "section": "Question #4:",
    "text": "Question #4:\nSort the DataFrame by “Fiscal_Year” and “Total_Other_Pay” in descending order, then set “First_Name” as the index and use the loc accessor to retrieve the “Total_Other_Pay” for a specific first name, say “MICHAEL”.\n\nQ4 = (\n    nyc_payroll\n    .sort_values([\"Fiscal_Year\",\"Total_Other_Pay\"], ascending = [False, False])\n    .set_index(\"First_Name\")\n    .loc[\"MICHAEL\"]\n)\nQ4\n\n\n  \n    \n\n\n\n\n\n\nFiscal_Year\nPayroll_Number\nAgency_Name\nLast_Name\nMid_Init\nAgency_Start_Date\nWork_Location_Borough\nTitle_Description\nLeave_Status_as_of_June_30\nBase_Salary\nPay_Basis\nRegular_Hours\nRegular_Gross_Paid\nOT_Hours\nTotal_OT_Paid\nTotal_Other_Pay\n\n\nFirst_Name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMICHAEL\n2023\n56.0\nPOLICE DEPARTMENT\nKERRIGAN\nS\n07/10/2006\nQUEENS\nLIEUTENANT\nACTIVE\n135511.00\nper Annum\n2080.00\n135139.69\n332.92\n38026.79\n27959.35\n\n\nMICHAEL\n2023\n56.0\nPOLICE DEPARTMENT\nDEASE\nP\n01/07/2008\nBROOKLYN\nPOLICE OFFICER\nACTIVE\n101590.00\nper Annum\n2080.00\n85058.22\n306.83\n22833.02\n17884.91\n\n\nMICHAEL\n2023\n57.0\nFIRE DEPARTMENT\nPRESTOVINO\nX\n03/31/2008\nRICHMOND\nFIRE ALARM DISPATCHER\nACTIVE\n68214.00\nper Annum\n2080.00\n68027.00\n57.00\n2950.01\n9331.02\n\n\nMICHAEL\n2023\n996.0\nNYC HOUSING AUTHORITY\nBIRTHWRIGHT\nV\n06/07/2021\nBRONX\nCARETAKER\nACTIVE\n32950.00\nper Annum\n2080.00\n32354.32\n489.75\n13205.47\n2529.38\n\n\nMICHAEL\n2023\n742.0\nDEPT OF ED PEDAGOGICAL\nGEORGE\nH\n09/07/1999\nMANHATTAN\nTEACHER\nACTIVE\n128657.00\nper Annum\n0.00\n128657.04\n0.00\n0.00\n475.00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nMICHAEL\n2014\n465.0\nCOMMUNITY COLLEGE (KINGSBORO)\nROSSON\nNaN\n07/01/1984\nNaN\nPROFESSOR\nACTIVE\n116364.00\nper Annum\n1195.00\n116045.27\n0.00\n0.00\n0.00\n\n\nMICHAEL\n2014\n747.0\nDEPT OF ED PER SESSION TEACHER\nSCHULMAN\nJ\n12/31/1999\nNaN\nTEACHER- PER SESSION\nACTIVE\n33.18\nper Day\n0.00\n1347.56\n0.00\n0.00\n0.00\n\n\nMICHAEL\n2014\n846.0\nDEPT OF PARKS & RECREATION\nMALONEY\nNaN\n06/14/2013\nNaN\nJOB TRAINING PARTICIPANT\nACTIVE\n9.21\nper Hour\n1835.17\n17417.65\n49.00\n896.82\n0.00\n\n\nMICHAEL\n2014\n747.0\nDEPT OF ED PER SESSION TEACHER\nMULSTAY\nNaN\n10/12/2004\nNaN\nTEACHER- PER SESSION\nACTIVE\n33.18\nper Day\n0.00\n10383.06\n0.00\n0.00\n0.00\n\n\nMICHAEL\n2014\n846.0\nDEPT OF PARKS & RECREATION\nBOOKMAN\nNaN\n04/03/2014\nNaN\nJOB TRAINING PARTICIPANT\nCEASED\n9.21\nper Hour\n212.00\n1952.53\n0.00\n0.00\n0.00\n\n\n\n\n74 rows × 16 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-5",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-5",
    "title": "- Part 1 -",
    "section": "Question #5:",
    "text": "Question #5:\nSort the DataFrame first by “Work_Location_Borough” alphabetically, and then by “Total_Compensation” (sum of “Base_Salary” and “Total_OT_Paid”) in descending order within each borough.\n\nQ4 = nyc_payroll\nQ4[\"Total_Compensation\"] = Q4[\"Base_Salary\"] + Q4[\"Total_OT_Paid\"]\nQ4 = (\n    nyc_payroll\n    .sort_values([\"Work_Location_Borough\",\"Total_Compensation\"], ascending = [True, False])\n)\nQ4\n\n\n  \n    \n\n\n\n\n\n\nFiscal_Year\nPayroll_Number\nAgency_Name\nLast_Name\nFirst_Name\nMid_Init\nAgency_Start_Date\nWork_Location_Borough\nTitle_Description\nLeave_Status_as_of_June_30\nBase_Salary\nPay_Basis\nRegular_Hours\nRegular_Gross_Paid\nOT_Hours\nTotal_OT_Paid\nTotal_Other_Pay\nTotal_Compensation\n\n\n\n\n3813\n2022\n56.0\nPOLICE DEPARTMENT\nWITRIOL\nJOEL\nNaN\n07/20/2006\nBRONX\nLIEUTENANT\nACTIVE\n135511.0\nper Annum\n2080.0\n135139.68\n919.0\n55080.42\n27374.03\n190591.42\n\n\n2068\n2021\n56.0\nPOLICE DEPARTMENT\nNUNEZ\nOSVALDO\nA\n07/18/1996\nBRONX\nCAPTAIN D/A DEPUTY INSPECTOR\nACTIVE\n180327.0\nper Annum\n2080.0\n179819.41\n0.0\n0.00\n26914.90\n180327.00\n\n\n1873\n2021\n11.0\nBOROUGH PRESIDENT-BRONX\nDIAZ JR\nRUBEN\nNaN\n05/01/2009\nBRONX\nBOROUGH PRESIDENT\nACTIVE\n179200.0\nper Annum\n1820.0\n178695.51\n0.0\n0.00\n1000.00\n179200.00\n\n\n1677\n2019\n902.0\nBRONX DISTRICT ATTORNEY\nOLDS\nVICTOR\nNaN\n09/07/2016\nBRONX\nASSISTANT DISTRICT ATTORNEY\nCEASED\n177400.0\nper Annum\n1267.0\n131483.29\n0.0\n0.00\n0.00\n177400.00\n\n\n864\n2022\n827.0\nDEPARTMENT OF SANITATION\nBRANNIGAN\nDESMOND\nG\n11/15/2004\nBRONX\nSANITATION WORKER\nACTIVE\n83465.0\nper Annum\n2080.0\n82186.02\n1171.0\n91463.76\n21167.33\n174928.76\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3881\n2014\n300.0\nBOARD OF ELECTION POLL WORKERS\nAREM\nROSE\nNaN\n01/01/2010\nNaN\nELECTION WORKER\nACTIVE\n1.0\nper Hour\n0.0\n500.00\n0.0\n0.00\n0.00\n1.00\n\n\n4223\n2014\n300.0\nBOARD OF ELECTION POLL WORKERS\nGATTO\nMARIE\nNaN\n01/01/2010\nNaN\nELECTION WORKER\nACTIVE\n1.0\nper Hour\n0.0\n778.00\n0.0\n0.00\n0.00\n1.00\n\n\n4278\n2014\n300.0\nBOARD OF ELECTION POLL WORKERS\nJOHNSON\nIRA\nN\n01/01/2010\nNaN\nELECTION WORKER\nACTIVE\n1.0\nper Hour\n0.0\n778.00\n0.0\n0.00\n0.00\n1.00\n\n\n4891\n2014\n300.0\nBOARD OF ELECTION POLL WORKERS\nCUNNINGHAM\nJEROME\nC\n01/02/2012\nNaN\nELECTION WORKER\nACTIVE\n1.0\nper Hour\n0.0\n305.00\n0.0\n0.00\n0.00\n1.00\n\n\n4940\n2014\n300.0\nBOARD OF ELECTION POLL WORKERS\nKISIC\nMICHELLE\nM\n01/02/2012\nNaN\nELECTION WORKER\nACTIVE\n1.0\nper Hour\n0.0\n303.00\n0.0\n0.00\n0.00\n1.00\n\n\n\n\n5000 rows × 18 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-6",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-6",
    "title": "- Part 1 -",
    "section": "Question #6:",
    "text": "Question #6:\nSelect employees who have “OT_Hours” &gt; 0, calculate their “OT_Rate” (“Total_OT_Paid” / “OT_Hours”), and then find the employee with the highest “OT_Rate”. Display their full name and “OT_Rate”.\n\nQ6 = nyc_payroll.query(\"OT_Hours &gt; 0\")\nQ6[\"OT_Rate\"] = Q6[\"Total_OT_Paid\"] / Q6[\"OT_Hours\"]\nQ6 = (\n    Q6\n    .sort_values(\"OT_Rate\", ascending = False)\n    .head(1)\n)\nQ6_Answer = Q6[[\"First_Name\",\"Last_Name\",\"OT_Rate\"]]\nQ6_Answer\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  Q6[\"OT_Rate\"] = Q6[\"Total_OT_Paid\"] / Q6[\"OT_Hours\"]\n\n\n\n  \n    \n\n\n\n\n\n\nFirst_Name\nLast_Name\nOT_Rate\n\n\n\n\n4224\nSHARON\nGOMEZ\n155.836"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-7",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-7",
    "title": "- Part 1 -",
    "section": "Question #7:",
    "text": "Question #7:\nCreate a new DataFrame that includes employees from the “DEPARTMENT OF EDUCATION ADMIN” agency where the variables are “First_Name”, “Last_Name”, “Title_Description”, “Base_Salary”, and “Total_OT_Paid”. Additionally, include a new variable “Total_Compensation” which is the sum of “Base_Salary” and “Total_OT_Paid”.\n\nDeptEdAdmin = nyc_payroll\nDeptEdAdmin[\"Total_Compensation\"] = Q4[\"Base_Salary\"] + Q4[\"Total_OT_Paid\"]\nDeptEdAdmin = (\n    Q7\n    .query(\"Agency_Name == 'DEPARTMENT OF EDUCATION ADMIN'\")\n)\nDeptEdAdmin = DeptEdAdmin[[\"First_Name\",\"Last_Name\", \"Title_Description\", \"Base_Salary\", \"Total_OT_Paid\",\"Total_Compensation\"]]\nDeptEdAdmin\n\n\n  \n    \n\n\n\n\n\n\nFirst_Name\nLast_Name\nTitle_Description\nBase_Salary\nTotal_OT_Paid\nTotal_Compensation\n\n\n\n\n79\nBETH\nSHIELS\nPHYSICAL THERAPIST\n81186.0\n0.00\n81186.00\n\n\n318\nSWETA\nGANDHI\nBOOKKEEPER\n47372.0\n16722.97\n64094.97\n\n\n340\nCLARE\nPURCELL\nSECRETARY\n45384.0\n0.00\n45384.00\n\n\n401\nNYESHA\nEVANS\nCUSTOMER INFORMATION REPRESENTATIVE\n67683.0\n19.79\n67702.79\n\n\n443\nSAMANTHA\nELIAS\nCOMMUNITY COORDINATOR\n76385.0\n0.00\n76385.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4798\nRENEI\nJOHNSON\nCOMMUNITY ASSISTANT\n29318.0\n48.14\n29366.14\n\n\n4835\nMICHELLE\nTIFLINSKY\nSENIOR OCCUPATIONAL THERAPIST\n64040.0\n9.72\n64049.72\n\n\n4870\nREBEKHA\nASKEW\nADMINISTRATIVE EDUCATION OFFICER\n108841.0\n0.00\n108841.00\n\n\n4903\nNICOLA\nSINCLAIR\nOCCUPATIONAL THERAPIST\n81186.0\n37.48\n81223.48\n\n\n4942\nELLEN\nMORELLO\nPHYSICAL THERAPIST\n67888.0\n0.00\n67888.00\n\n\n\n\n135 rows × 6 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-8",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-8",
    "title": "- Part 1 -",
    "section": "Question #8:",
    "text": "Question #8:\nHow many employees have a “Base_Salary” within the top 10% of the DataFrame?\n\nTotal_Employee_Count = len(nyc_payroll)\nTotal_Employee_Count\n\nEmployee_Subtraction = (\n    nyc_payroll\n    .sort_values(\"Base_Salary\", ascending = False)\n)\nPercentile = Employee_Subtraction[\"Base_Salary\"].quantile(.9)\nEmployee_Subtraction = (\n    Employee_Subtraction[\"Base_Salary\"] &lt; Percentile\n)\nEmployee_Subtraction = Employee_Subtraction.sum()\nEmployee_Subtraction\n\n# After doing all this, I see that the percentile could've just came from the length of the payroll multiplied by .1 (Representing 1/10 of the data)\n\n4500"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-9",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-9",
    "title": "- Part 1 -",
    "section": "Question #9:",
    "text": "Question #9:\nFilter the DataFrame for employees who have “OT_Hours” greater than 0 but less than 100, and their “Leave_Status_as_of_June_30” is “ACTIVE”.\n\nQ9 = (\n    nyc_payroll\n    .query(\"OT_Hours &gt; 0 and OT_Hours &lt; 100 and Leave_Status_as_of_June_30 == 'ACTIVE'\")\n)\nQ9 # Seeing this after all the excess work I did in the last one made me a little more confident\n\n\n  \n    \n\n\n\n\n\n\nFiscal_Year\nPayroll_Number\nAgency_Name\nLast_Name\nFirst_Name\nMid_Init\nAgency_Start_Date\nWork_Location_Borough\nTitle_Description\nLeave_Status_as_of_June_30\nBase_Salary\nPay_Basis\nRegular_Hours\nRegular_Gross_Paid\nOT_Hours\nTotal_OT_Paid\nTotal_Other_Pay\nTotal_Compensation\n\n\n\n\n6\n2014\n841.0\nDEPARTMENT OF TRANSPORTATION\nLOMUNTAD\nLUIS\nV\n11/28/2004\nNaN\nDECKHAND\nACTIVE\n49793.0\nper Annum\n1668.38\n49650.65\n72.00\n7289.37\n3819.73\n57082.37\n\n\n39\n2015\nNaN\nFIRE DEPARTMENT\nCIRINO\nVINCENT\nC\n10/15/1990\nMANHATTAN\nCAPTAIN\nACTIVE\n117145.0\nper Annum\n2085.72\n121939.16\n14.00\n1305.47\n20076.23\n118450.47\n\n\n71\n2020\n906.0\nDISTRICT ATTORNEY-SPECIAL NARC\nNaN\nNaN\nNaN\n06/11/2018\nMANHATTAN\nSENIOR RACKETS INVESTIGATOR - START &gt;4-24-08 N...\nACTIVE\n67012.0\nper Annum\n2080.00\n64551.27\n27.00\n1290.10\n7993.00\n68302.10\n\n\n91\n2019\n806.0\nHOUSING PRESERVATION & DVLPMNT\nSALAS\nJESSICA\nA\n04/19/2004\nMANHATTAN\nADMIN COMMUNITY RELATIONS SPECIALIST\nACTIVE\n58926.0\nper Annum\n1809.25\n58681.36\n30.75\n1487.66\n135.71\n60413.66\n\n\n104\n2020\n806.0\nHOUSING PRESERVATION & DVLPMNT\nDESTINOBLE-NEUS\nANNE-LOISE\nD\n01/06/2020\nMANHATTAN\nCOMMUNITY ASSOCIATE\nACTIVE\n38333.0\nper Annum\n840.00\n17443.61\n7.00\n167.85\n0.00\n38500.85\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4903\n2022\n740.0\nDEPARTMENT OF EDUCATION ADMIN\nSINCLAIR\nNICOLA\nS\n09/08/2009\nQUEENS\nOCCUPATIONAL THERAPIST\nACTIVE\n81186.0\nper Annum\n1664.00\n80815.89\n0.50\n37.48\n14172.85\n81223.48\n\n\n4911\n2021\n56.0\nPOLICE DEPARTMENT\nQUINONES\nCLAIRE\nNaN\n07/06/2011\nMANHATTAN\nPOLICE OFFICER\nACTIVE\n85292.0\nper Annum\n2080.00\n85051.84\n87.25\n4022.93\n19385.83\n89314.93\n\n\n4949\n2020\n827.0\nDEPARTMENT OF SANITATION\nFIGUEROA\nRICARDO\nA\n10/09/2007\nBROOKLYN\nSANITATION WORKER\nACTIVE\n77318.0\nper Annum\n2080.00\n70356.48\n32.00\n2293.47\n3729.19\n79611.47\n\n\n4979\n2015\nNaN\nPOLICE DEPARTMENT\nMARGOLIS\nJASON\nS\n07/18/1996\nQUEENS\nLIEUTENANT\nACTIVE\n117145.0\nper Annum\n2085.72\n119504.86\n8.75\n1100.95\n25296.48\n118245.95\n\n\n4992\n2018\n69.0\nHRA/DEPT OF SOCIAL SERVICES\nPECHERA\nJAMES\nO\n04/25/2011\nQUEENS\nJOB OPPORTUNITY SPECIALIST\nACTIVE\n45378.0\nper Annum\n1410.75\n35823.90\n6.25\n222.59\n2036.87\n45600.59\n\n\n\n\n339 rows × 18 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-10",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-10",
    "title": "- Part 1 -",
    "section": "Question #10:",
    "text": "Question #10:\nFind the unique job titles in the “DEPARTMENT OF EDUCATION ADMIN” agency and count how many there are.\n\nDeptEdAdmin = nyc_payroll\nDeptEdAdmin = (\n    Q7\n    .query(\"Agency_Name == 'DEPARTMENT OF EDUCATION ADMIN'\")\n)\nDeptEdAdmin = DeptEdAdmin[[\"Title_Description\"]]\nNum_of_departments = DeptEdAdmin.nunique()\n\nDeptCounts = DeptEdAdmin.value_counts()\n\nprint(\"There is: \",Num_of_departments, \" Departments.\") #idk how to get rid of the item like \"title description\"\nprint(DeptCounts)\n\nThere is:  Title_Description    37\ndtype: int64  Departments.\nTitle_Description                                       \nOCCUPATIONAL THERAPIST                                      28\nCOMMUNITY ASSOCIATE                                         16\nSTAFF NURSE                                                 14\nPHYSICAL THERAPIST                                           9\nCOMMUNITY COORDINATOR                                        9\nADMINISTRATIVE EDUCATION ANALYST                             7\nSCHOOL LUNCH AIDE                                            6\nADMINISTRATIVE EDUCATION OFFICER                             5\nSCHOOL COMPUTER TECHNOLOGY SPECIALIST                        4\nCOMMUNITY ASSISTANT                                          3\nSCHOOL BUSINESS MANAGER                                      2\nSCHOOL FOOD SERVICE MANAGER                                  2\nEDUCATION OFFICER                                            2\nCLERICAL ASSOCIATE                                           2\nSENIOR OCCUPATIONAL THERAPIST                                2\nSENIOR SCHOOL LUNCH AIDE                                     2\nADMINISTRATIVE STAFF ANALYST                                 2\nPRINCIPAL ADMINISTRATIVE ASSOCIATE - LEV 1 & 2 NON SUPVR     1\nSCHOOL LUNCH ASSISTANT COOK                                  1\nSECRETARY                                                    1\nSUBSTANCE ABUSE PREVENTION & INTERVENTION SPECIALIST         1\nSUPERVISING THERAPIST                                        1\nPROCUREMENT ANALYST                                          1\nACCOUNTANT                                                   1\nPLASTERER                                                    1\nEXTERMINATOR                                                 1\nCUSTOMER INFORMATION REPRESENTATIVE                          1\nCONFIDENTIAL INVESTIGATOR                                    1\nCOMPUTER SPECIALIST                                          1\nCOMPUTER SERVICE TECHNICIAN                                  1\nBOOKKEEPER                                                   1\nASSOCIATE RETIREMENT BENEFITS EXAMINER                       1\nASSOCIATE QUALITY ASSURANCE SPECIALIST                       1\nASSOCIATE INVESTIGATOR                                       1\nAGENCY ATTORNEY                                              1\nADMINISTRATIVE PROCUREMENT ANALYST                           1\nSUPERVISOR OF NURSES                                         1\nName: count, dtype: int64"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-11",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-11",
    "title": "- Part 1 -",
    "section": "Question #11:",
    "text": "Question #11:\n\nIdentify the employee(s) with the highest “Total_OT_Paid” in the DataFrame.\nInclude their “First_Name”, “Last_Name”, and “Total_OT_Paid”.\n\n\nQ11 = (\n    nyc_payroll\n    .sort_values(\"Total_OT_Paid\", ascending = False)\n    .head(1)\n)\nQ11[[\"First_Name\",\"Last_Name\",'Total_OT_Paid']]\n\n\n  \n    \n\n\n\n\n\n\nFirst_Name\nLast_Name\nTotal_OT_Paid\n\n\n\n\n1228\nDANIEL\nHOCK, JR.\n123664.33"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-12",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-12",
    "title": "- Part 1 -",
    "section": "Question #12:",
    "text": "Question #12:\n\nWhat percentage of the values is missing for each variable?\n\n\nNon_Null_Values = nyc_payroll.info()\nTotalValues = len(nyc_payroll)\n\nprint(Non_Null_Values, TotalValues)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 18 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Fiscal_Year                 5000 non-null   int64  \n 1   Payroll_Number              3460 non-null   float64\n 2   Agency_Name                 5000 non-null   object \n 3   Last_Name                   4985 non-null   object \n 4   First_Name                  4985 non-null   object \n 5   Mid_Init                    2920 non-null   object \n 6   Agency_Start_Date           5000 non-null   object \n 7   Work_Location_Borough       4602 non-null   object \n 8   Title_Description           5000 non-null   object \n 9   Leave_Status_as_of_June_30  5000 non-null   object \n 10  Base_Salary                 5000 non-null   float64\n 11  Pay_Basis                   5000 non-null   object \n 12  Regular_Hours               5000 non-null   float64\n 13  Regular_Gross_Paid          5000 non-null   float64\n 14  OT_Hours                    5000 non-null   float64\n 15  Total_OT_Paid               5000 non-null   float64\n 16  Total_Other_Pay             5000 non-null   float64\n 17  Total_Compensation          5000 non-null   float64\ndtypes: float64(8), int64(1), object(9)\nmemory usage: 703.3+ KB\nNone 5000"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-13",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-13",
    "title": "- Part 1 -",
    "section": "Question #13:",
    "text": "Question #13:\n\nFill missing values in the “Last_Name” variable with “UNKNOWN”.\n\n\nChangeName = nyc_payroll.fillna(\"UNKNOWN\")\nChangeName[[\"Last_Name\"]]\n# For loop wasn't working so I looked up how to use .fillna()\nChangeName = ChangeName[[\"Last_Name\"]]\n\nunknown_count = ChangeName['Last_Name'].value_counts()['UNKNOWN']\nunknown_count # This is optional, just to check that it actually worked\n\n15"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-14",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-14",
    "title": "- Part 1 -",
    "section": "Question #14:",
    "text": "Question #14:\nIn DataFrame, NFL2022_stuffs, remove observations for which the value of posteam is missing.\n\nQ14 = (\n    NFL2022_stuffs\n    .query(\"posteam.notnull()\")\n ) # This I also looked up, my other attempt was a really long if statement that started to confuse me about what I was even writing\nQ14\n\n\n  \n    \n\n\n\n\n\n\nplay_id\ngame_id\ndrive\nweek\nposteam\nqtr\ndown\nhalf_seconds_remaining\npass\nwp\n\n\n\n\n1\n43\n2022_01_BAL_NYJ\n1.0\n1\nNYJ\n1\nNaN\n1800\n0\n0.546262\n\n\n2\n68\n2022_01_BAL_NYJ\n1.0\n1\nNYJ\n1\n1.0\n1796\n0\n0.546969\n\n\n3\n89\n2022_01_BAL_NYJ\n1.0\n1\nNYJ\n1\n1.0\n1769\n1\n0.572573\n\n\n4\n115\n2022_01_BAL_NYJ\n1.0\n1\nNYJ\n1\n2.0\n1765\n0\n0.554537\n\n\n5\n136\n2022_01_BAL_NYJ\n1.0\n1\nNYJ\n1\n3.0\n1741\n1\n0.540167\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50139\n3968\n2022_22_KC_PHI\n17.0\n22\nKC\n4\n2.0\n96\n0\n0.813623\n\n\n50141\n3996\n2022_22_KC_PHI\n17.0\n22\nKC\n4\n3.0\n54\n0\n0.817793\n\n\n50143\n4024\n2022_22_KC_PHI\n17.0\n22\nKC\n4\n4.0\n11\n0\n0.782616\n\n\n50144\n4050\n2022_22_KC_PHI\n18.0\n22\nPHI\n4\nNaN\n8\n0\n0.087426\n\n\n50145\n4072\n2022_22_KC_PHI\n18.0\n22\nPHI\n4\n1.0\n6\n1\n0.127170\n\n\n\n\n46427 rows × 10 columns"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-15",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-15",
    "title": "- Part 1 -",
    "section": "Question #15:",
    "text": "Question #15:\n\nCalculate the mean value of pass for the BUF posteam when all the following conditions hold:\nwp is greater than 20% and less than 75%;\ndown is less than or equal to 2; and\nhalf_seconds_remaining is greater than 120.\n\n\nQ15 = (\n    NFL2022_stuffs\n    .query(\"posteam == 'BUF' and wp &gt; .2 and wp &lt; .75 and down &lt;= 2 and half_seconds_remaining &gt; 120\")\n)\n\nQ15Mean = Q15[\"pass\"].mean()\nQ15Mean\n\n0.6043956043956044"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-16",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-16",
    "title": "- Part 1 -",
    "section": "Question #16:",
    "text": "Question #16:\n\nConsider the following DataFrame, NFL2022_epa:\n\n\nNFL2022_epa = pd.read_csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\n\nCreate the following DataFrame, NFL2022_stuffs_EPA, that includes\nAll the variables and the observations in the DataFrame, NFL2022_stuffs;\nThe variables, passer, receiver, and epa, from the DataFrame, NFL2022_epa by joining the two DataFrames.\nIn the resulting DataFrame, NFL2022_stuffs_EPA, please remove observations with NaN in passer after the join.\n\n\nNFL2022_stuffs_EPA = (\n    pd.merge(NFL2022_stuffs,NFL2022_epa, on = \"posteam\", how = \"left\")\n)\nNFL2022_stuffs_EPA"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-17",
    "href": "PythonNotebooks/danl_210_hw2_Taratko_Christopher.html#question-17",
    "title": "- Part 1 -",
    "section": "Question #17:",
    "text": "Question #17:\n\nReshape the trashwheel DataFrame into a DataFrame called trashwheel_long that includes variables for “Name”, “Date”, “Dumpster”, “Trash_Type”, and “Number”.\n\nThe “Trash_Type” variable should indicate the type of trash from the original DataFrame, and “Number” should contain the corresponding values.\nFinally, sort trashwheel_long by “Name”, “Date”, “Dumpster”, and “Trash_Type” in ascending order.\nThe following displays the trashwheel_long DataFrame:\n\n\n\ntrashwheel_long = pd.melt(\n    trashwheel,\n    id_vars=[\"Name\", \"Date\", \"Dumpster\"],  # Keep these columns fixed\n    var_name=\"Trash_Type\",  # Create a new column for trash types\n    value_name=\"Number\"  # Store corresponding values here\n)\ntrashwheel_long = trashwheel_long.sort_values([\"Name\", \"Date\", \"Dumpster\", \"Trash_Type\"], ascending = [True,True,True,True])\ntrashwheel_long\n\n\n  \n    \n\n\n\n\n\n\nName\nDate\nDumpster\nTrash_Type\nNumber\n\n\n\n\n7705\nCaptain Trash Wheel\n1/30/20\n12\nCigaretteButts\n3300.0\n\n\n8698\nCaptain Trash Wheel\n1/30/20\n12\nGlassBottles\nNaN\n\n\n12670\nCaptain Trash Wheel\n1/30/20\n12\nHomesPowered\n14\n\n\n754\nCaptain Trash Wheel\n1/30/20\n12\nID\ncaptain\n\n\n1747\nCaptain Trash Wheel\n1/30/20\n12\nMonth\nJanuary\n\n\n...\n...\n...\n...\n...\n...\n\n\n11647\nProfessor Trash Wheel\n9/9/22\n96\nSportsBalls\nNaN\n\n\n4696\nProfessor Trash Wheel\n9/9/22\n96\nVolume\n10\n\n\n3703\nProfessor Trash Wheel\n9/9/22\n96\nWeight\n1.97\n\n\n10654\nProfessor Trash Wheel\n9/9/22\n96\nWrappers\n4200.0\n\n\n2710\nProfessor Trash Wheel\n9/9/22\n96\nYear\n2022\n\n\n\n\n12909 rows × 5 columns"
  },
  {
    "objectID": "Analytics200Review.html",
    "href": "Analytics200Review.html",
    "title": "Lab Exercises - DANL 200",
    "section": "",
    "text": "Data Analytics Lab Review (In Class Exercises)\nThese Lab Exercises consist of if statements, for loops, while loops, and repeat statements\n\n\n\n\n1 Lab 7\n\n# IN-CLASS EXERCISE\n#   Create an if-else statement example in R that assigns a hypothetical starting salary based on different education levels. We’ll consider three levels of education: High School, Bachelor’s Degree, and Master’s Degree.\n\n#In this example:\n\n#   If the education level is “High School”, the starting salary is set to $30,000.\n#   If the education level is “Bachelor’s Degree”, the starting salary is set to $50,000.\n#   If the education level is “Master’s Degree”, the starting salary is set to $70,000.\n#   For any other education level not explicitly listed, the default starting salary is set to $40,000.\n\n# At the beginning, insert education level is “Bachelor’s Degree”, and Initialize the starting salary at 0. At the end use print() to show the result.\n\nEducation &lt;- \"Bachelor's Degree\"\nStarting_Salary &lt;- 0\nif (Education == \"High School\") {\n  Starting_Salary &lt;- 30000\n} else if (Education == \"Bachelor's Degree\") {\n  Starting_Salary &lt;- 50000\n} else if (Education == \"Master's Degree\") {\n  Starting_Salary &lt;- 70000\n} else {\n  Starting_Salary &lt;- 40000\n}\nprint(paste(\"This person has a \",Education,\" and their starting salary will be $\",Starting_Salary,sep = ''))\n\n[1] \"This person has a Bachelor's Degree and their starting salary will be $50000\"\n\n\n\n\n2 Lab 8\n\n#     In-Class Exercise: Adjusting Discount Rates Based on Purchase Amount\n# Exercise Description: Based on the previous example, now introduce an additional \n# condition: for premium members, if their purchase amount exceeds $200, they get \n# a 15% discount instead of 10%. all the rest are same, including non exceeds $200 \n# will get 10% and Non-premium members still do not receive any discount.\n\n\n# Purchase amounts and membership status\npurchase_amounts &lt;- c(100, 150, 200, 250, 300)\nis_premium_member &lt;- c(TRUE, FALSE, TRUE, FALSE, TRUE)\n\n# Initialize a vector to hold the final amounts, the initial value doesn't matter, eventually they will updated. But the length is important, it gives us the vector of results.\nfinal_amounts &lt;- numeric(length(purchase_amounts))\n\n# Use a for loop with if-else statement to apply discounts\nfor (i in 1:length(purchase_amounts)) {\n  if (is_premium_member[i] && purchase_amounts[i] &gt; 200) {\n    final_amounts[i] &lt;- purchase_amounts[i] * .85  # Applied 15% discount\n  } else if (is_premium_member[i]) {\n    final_amounts[i] &lt;- purchase_amounts[i] * 0.9  # Apply 10% discount\n  } else {\n    final_amounts[i] &lt;- purchase_amounts[i]  # No discount\n  } # My formatting (Works)\n}\n\n# for (i in 1:length(purchase_amounts)) \n#   if (is_premium_member[i]) {\n#     if (is_premium_member[i] && purchase_amounts[i] &gt; 200) {\n#       final_amounts[i] &lt;- purchase_amounts[i] * 0.85  # Applied 15% discount \n#     } else {\n#     final_amounts[i] &lt;- purchase_amounts[i] * 0.9  # Apply 10% discount\n#   }\n#     } else {\n#     final_amounts[i] &lt;- purchase_amounts[i]  # No discount\n#   } # Li Lu's Formatting (Cleaner Formatting, nested if statements)\n\nprint(final_amounts)\n\n[1]  90 150 180 250 255\n\n\n\n\n3 Lab 9\n\n# In-Class Exercise: Inventory for a popular book\n# Exercise Description: You own a small bookstore, and you keep track of the inventory for a popular book. Initially, you have 50 books in stock. Throughout the day, customers buy them, and you record each sale. Your policy is to order 30 more books whenever your stock drops below 10 to ensure you always have enough books to meet demand. The simulation stops after ordering more books once, for simplicity.\n\n\n# Initial Setup\nInitial_Stock &lt;- 50\nOrderThreshold &lt;- 10\nOrderQ &lt;- 30\nCurrentStock &lt;- Initial_Stock\nrepeat {\n  while (CurrentStock &gt; OrderThreshold) {\n  Q_Book_Sold &lt;- sample(1:5,1)\n  CurrentStock &lt;- CurrentStock - Q_Book_Sold\n  print(paste(\"The initial Stock of\",Initial_Stock,\"has been reduced to:\",CurrentStock))\n  } \n  if (CurrentStock &lt;= OrderThreshold) {\n  print(paste(\"Stock originally was at:\", CurrentStock))\n  CurrentStock &lt;- CurrentStock + OrderQ\n  print(paste(\"The updated Stock is now at:\", CurrentStock))\n  break\n  } \n}\n\n[1] \"The initial Stock of 50 has been reduced to: 45\"\n[1] \"The initial Stock of 50 has been reduced to: 43\"\n[1] \"The initial Stock of 50 has been reduced to: 38\"\n[1] \"The initial Stock of 50 has been reduced to: 33\"\n[1] \"The initial Stock of 50 has been reduced to: 32\"\n[1] \"The initial Stock of 50 has been reduced to: 28\"\n[1] \"The initial Stock of 50 has been reduced to: 23\"\n[1] \"The initial Stock of 50 has been reduced to: 19\"\n[1] \"The initial Stock of 50 has been reduced to: 17\"\n[1] \"The initial Stock of 50 has been reduced to: 14\"\n[1] \"The initial Stock of 50 has been reduced to: 10\"\n[1] \"Stock originally was at: 10\"\n[1] \"The updated Stock is now at: 40\"\n\n\n\n\n4 Another Class Exercise\n\nConsumerPoints &lt;- 85\nwhile (ConsumerPoints &lt; 140) {\n  if (ConsumerPoints &lt; 100) {\n    ConsumerPoints &lt;- ConsumerPoints + 5\n    print(ConsumerPoints)\n  } else if (ConsumerPoints &gt;= 100) {\n    ConsumerPoints &lt;- ConsumerPoints + 5\n    print(paste(\"You have\", ConsumerPoints, \"Points!\",\"\"))\n    readline(\"Would you like to Convert your Points to Cash: [Y/N] \")\n    break\n  }\n}\n\n[1] 90\n[1] 95\n[1] 100\n[1] \"You have 105 Points! \"\nWould you like to Convert your Points to Cash: [Y/N] \n\n\n\n\n[1] \"This Website has been fixed\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Taratko",
    "section": "",
    "text": "About Me\n\nI have majors in\nAccounting & Economics\n,with a Minor in\nData Analytics\nat SUNY Geneseo. When not working on college assignments, I enjoy sailing, snowboarding, zip-lining, rock climbing and gaming. Other interesting facts about me is that I have a Level 1 Sailing Certification, I race sailboats competitively through the summer and winter seasons, and my hopes for my college education is to bring a balance within Data Analytics and Accounting. I see both of these majors as essential to each other, especially now that AI will cause job uncertainty in programmable positions and is being developed to predict markets.\n\nEducation\nState University of New York at Geneseo | Geneseo, NY  B.S in Accounting & Economics | Aug 2022 - May 2026 Minor in Data Analytics\nExperience\nDaidone Income Tax LLC - Mentorship\nLinkedIn Learning - Excel Tutorial\nLinkedIn Learning - Intermediate Excel\nLinkedIn Learning - Excel - Financial Functions in Depth\nLinkedIn Learning - Managing and Analyzing Data in Excel"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Let’s analyze the beer_data data:\nbeer_data &lt;- read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "href": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "title": "Beer Markets",
    "section": "Variable Description for beer_data data.frame",
    "text": "Variable Description for beer_data data.frame\nThe following describes the variables in the beer_data data.frame.\n\nhh: Household identifier\n_purchase_desc: Description of the purchase\nquantity: The quantity of beer purchased\nbrand: The brand of beer\ndollar_spent: The amount spent\nbeer_floz: Fluid ounces of beer\nprice_per_floz: Price per fluid ounce\ncontainer: Type of container\npromo: Whether the purchase was on promotion\nmarket: The market where the purchase was made\nDemographics: age, employment status, degree, class of worker (cow), race, and household information like microwave, dishwasher, tvcable, singlefamilyhome, and npeople (number of people in the household)"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "href": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "title": "Beer Markets",
    "section": "Purchase Patterns",
    "text": "Purchase Patterns\nWe’ll explore the purchase patterns in the dataset. This includes understanding the most popular brands, the average quantity purchased, and spending habits across different markets. Here are some specific analyses we can perform:\n\nCalculate the total quantity and spending for each brand.\nFind the average quantity purchased and average spending per purchase.\nCompare the total spending across different markets.\n\nI’ll begin with these analyses and create visualizations to help us understand the data better. Let’s start by calculating the total quantity and spending for each brand.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\n\n# Setting up the visualisation settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\n\n\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n# Plotting total spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=brand_summary_sorted_spent, palette='viridis')\nplt.title('Total Spending on Beer by Brand')\nplt.xlabel('Total Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let’s calculate the average quantity purchased and average spending per purchase. For this, we’ll consider each row in the dataset as a separate purchase and compute the averages accordingly.\n\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean', \n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n# Plotting average spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=average_purchase_sorted_spent, palette='viridis')\nplt.title('Average Spending on Beer by Brand')\nplt.xlabel('Average Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we’ll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we’ll sum up the spending in each market and visualize it.\n\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 10))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let’s move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI’ll start by analyzing spending by age group.\n\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let’s look at spending by race to complete the demographic analysis.\n\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let’s proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe’ll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we’ll calculate the average price per fluid ounce for each brand and then visualize how this relates to the average quantity purchased and the total quantity purchased by brand.\nFirst, let’s calculate the average price per fluid ounce for each brand.\n\n# Calculate average price per fluid ounce for each brand\nbrand_price_sensitivity = beer_data.groupby('brand').agg({\n    'price_per_floz': 'mean', \n    'quantity': 'sum'\n}).reset_index()\n\n# Sort by price per fluid ounce\nbrand_price_sensitivity_sorted = brand_price_sensitivity.sort_values('price_per_floz', ascending=True)\n\n# Plotting average price per fluid ounce for each brand and the total quantity purchased\nfig, ax1 = plt.subplots(figsize=(12, 10))\n\ncolor = 'tab:red'\nax1.set_xlabel('Brand')\nax1.set_ylabel('Average Price per Fluid Ounce', color=color)\nax1.bar(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['price_per_floz'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax1.set_xticklabels(brand_price_sensitivity_sorted['brand'], rotation=90)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\ncolor = 'tab:blue'\nax2.set_ylabel('Total Quantity Purchased', color=color)  # we already handled the x-label with ax1\nax2.plot(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['quantity'], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.title('Average Price per Fluid Ounce & Total Quantity Purchased by Brand')\nplt.show()\n\n\n\n\n\n\n\n\nIn the visualization, we have a bar graph showing the average price per fluid ounce for each brand (in red) and a line graph showing the total quantity purchased for each brand (in blue). This gives us a sense of whether there’s a relationship between the price and the quantity purchased. The x-axis labels are quite compressed due to the number of brands, but we can still observe trends such as whether lower-priced beers tend to be purchased in larger quantities.\nLastly, let’s move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact",
    "title": "Beer Markets",
    "section": "Promotional Impact",
    "text": "Promotional Impact\nWe’ll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We’ll do this for each brand to see which brands are most affected by promotions.\nLet’s begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn’t. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "NVDY Planner",
    "section": "",
    "text": "NVDY Dividend Reinvestment Code\n\nThe reinvestment planner for the ticker, NVDY, is based off of the stock ticker for Nvidia (NVDA), where ProShares performs options trading on behalf of the shareholders in order to generate profits for said shareholders and the company through the distribution of Stock prices and dividends payments which can vary greatly, but have performed well in the past year, allowing shareholders to receive 1.5x shares through just monthly dividend reinvestment in this past year.\nThe strategies used for these option trades can vary immensely, but would look as if they contain a self\n\nCode-Block:"
  },
  {
    "objectID": "IndexRenderDeploy.html",
    "href": "IndexRenderDeploy.html",
    "title": "Christopher Taratko",
    "section": "",
    "text": "About Me\n\nI have majors in\nAccounting & Economics\n,with a Minor in\nData Analytics\nat SUNY Geneseo. When not working on college assignments, I enjoy sailing, snowboarding, zip-lining, rock climbing and gaming. Other interesting facts about me is that I have a Level 1 Sailing Certification, I race sailboats competitively through the summer and winter seasons, and my hopes for my college education is to bring a balance within Data Analytics and Accounting. I see both of these majors as essential to each other, especially now that AI will cause job uncertainty in programmable positions and is being developed to predict markets.\n\nEducation\nState University of New York at Geneseo | Geneseo, NY  B.S in Accounting & Economics | Aug 2022 - May 2026 Minor in Data Analytics\nExperience\nDaidone Income Tax LLC - Mentorship\nLinkedIn Learning - Excel Tutorial\nLinkedIn Learning - Intermediate Excel\nLinkedIn Learning - Excel - Financial Functions in Depth\nLinkedIn Learning - Managing and Analyzing Data in Excel\n\n\n\n\n# #| echo: false\n# const DATA_ENTRY_SHEET_NAME = \"1BT-M6uJ5OdLBnojAMorrafl7VW6GBaSbZMJJD3h-_08\"; # 1BT-M6uJ5OdLBnojAMorrafl7VW6GBaSbZMJJD3h-_08\n# var sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(DATA_ENTRY_SHEET_NAME);\n# \n# const doPost = (request = {}) =&gt; {\n#   const { postData: { contents, type } = {} } = request;\n#   if (!contents) {\n#     console.error('No postData contents found');\n#     return ContentService.createTextOutput('No postData contents found').setMimeType(ContentService.MimeType.TEXT);\n#   }\n#   var data = parseFormData(contents);\n#   if (data) {\n#     appendToGoogleSheet(data);\n#     return ContentService.createTextOutput(JSON.stringify(data)).setMimeType(ContentService.MimeType.JSON);\n#   } else {\n#     return ContentService.createTextOutput('Error parsing form data').setMimeType(ContentService.MimeType.TEXT);\n#   }\n# };\n# \n# function parseFormData(postData) {\n#   if (typeof postData !== 'string') {\n#     console.error('postData is not a string:', postData);\n#     return null;\n#   }\n#   var data = {};\n#   var parameters = postData.split('&');\n#   for (var i = 0; i &lt; parameters.length; i++) {\n#     var [key, value] = parameters[i].split('=');\n#     data[key] = decodeURIComponent(value.replace(/\\+/g, ' '));\n#   }\n#   return data;\n# }\n# \n# function appendToGoogleSheet(data) {\n#   var headers = sheet.getRange(1, 1, 1, sheet.getLastColumn()).getValues()[0];\n#   var rowData = headers.map(headerFld =&gt; data[headerFld] || \"\");\n#   sheet.appendRow(rowData);\n# }\n# \n#"
  },
  {
    "objectID": "posts/Stock Code/index.html",
    "href": "posts/Stock Code/index.html",
    "title": "Stocks",
    "section": "",
    "text": "This is a post with executable code.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nStockPost &lt;- read_csv(\"/Users/christophertaratko/ProjectsList/Data Analytics/Personal Website/ctaratko.github.io/StocksUpdated2.csv\")\n\nRows: 25160 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Company, Date\ndbl (5): Close/Last, Volume, Open, High, Low\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nview(StockPost)"
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "quarto-template.html",
    "href": "quarto-template.html",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "",
    "text": "oj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nnvars &lt;- format(round(ncol(oj), 0), \n                nsmall=0, \n                big.mark=\",\") \nnobs &lt;- format(round(nrow(oj), 0), \n                nsmall=0, \n                big.mark=\",\")\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red\nviolets are blue."
  },
  {
    "objectID": "quarto-template.html#data-summary",
    "href": "quarto-template.html#data-summary",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.1 Data Summary",
    "text": "2.1 Data Summary\n\nSummary statistics (Use skimr::skim())"
  },
  {
    "objectID": "quarto-template.html#data-visualization",
    "href": "quarto-template.html#data-visualization",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\noj %&gt;% \n  ggplot(aes(x = log(sales), \n             y = log(price),\n             color = brand)) +\n  geom_point(alpha = .1) +\n  geom_smooth(method = lm, se = F) +\n  facet_wrap(.~ad) +\n  theme_bw() +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "quarto-template.html#data-transformation",
    "href": "quarto-template.html#data-transformation",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.3 Data Transformation",
    "text": "2.3 Data Transformation\n\nob_sum1 &lt;- oj %&gt;% \n  group_by(brand, ad) %&gt;% \n  summarise(sales_tot = sum(sales, na.rm = T),\n            price_mean = round(mean(price, na.rm = T), 2))"
  },
  {
    "objectID": "quarto-template.html#analysis",
    "href": "quarto-template.html#analysis",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.4 Analysis",
    "text": "2.4 Analysis"
  },
  {
    "objectID": "quarto-template.html#quotes",
    "href": "quarto-template.html#quotes",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.5 Quotes",
    "text": "2.5 Quotes\n\nQuote with &gt;\n\n\n“The truth is rarely pure and never simple.”\n— Oscar Wilde"
  },
  {
    "objectID": "quarto-template.html#inserting-figures",
    "href": "quarto-template.html#inserting-figures",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.6 Inserting Figures",
    "text": "2.6 Inserting Figures\nFor a demonstration of a DANL tiger, see Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: DANL Tiger"
  },
  {
    "objectID": "quarto-template.html#inserting-a-html-page",
    "href": "quarto-template.html#inserting-a-html-page",
    "title": "DANL 200: Christopher Taratko’s WorkspaceTeam Project",
    "section": "2.7 Inserting a HTML page",
    "text": "2.7 Inserting a HTML page"
  },
  {
    "objectID": "DANL210_Project.html#background",
    "href": "DANL210_Project.html#background",
    "title": "Data Analytics – [Project Name]",
    "section": "- Background",
    "text": "- Background\n\nProvide context for the research questions, explaining why they are significant, relevant, or interesting"
  },
  {
    "objectID": "DANL210_Project.html#statement-of-the-problem",
    "href": "DANL210_Project.html#statement-of-the-problem",
    "title": "Data Analytics – [Project Name]",
    "section": "- Statement of the Problem",
    "text": "- Statement of the Problem\n\n\n\n\n\n\nImportant\n\n\n\nThis is a callout, it’ll help define items on my page that might be important, or other such key features. You can find this at PositCo’s Cheatsheet page\n\n\n\nClearly articulate the specific problem or issue the project will address."
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Insightful Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNVDY Planner\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\nChristopher Taratko\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nByeong-Hak Choe\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nStocks\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nChristopher Taratko\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarwars\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nYour Name\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "PythonNotebooks/danl_210_hw1_Taratko_Christopher.html",
    "href": "PythonNotebooks/danl_210_hw1_Taratko_Christopher.html",
    "title": "Question 1:",
    "section": "",
    "text": "Question 0:\n\n# Providing Github Username:\n# Username: ctaratko\n\n\nQuestion 1:\nQ1(a). - Create a list of integers from 1 to 10 - Append the number 11 to the list and remove the number 5\n\nMyList = [1,2,3,4,5,6,7,8,9,10]\nMyList.append(11)\nMyList.remove(5)\nMyList\n\n[1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n\n\nQ1(b). - Consider the following Dictionary of three employees and their salaries:\n\ndict_salaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000}\n\n\nAdd a new employee 'Dana' with a salary of 65000\nUpdate 'Alice'’s Salary to 55000\nPrint all employee names and their salaries\n\n\ndict_salaries.update({'Dana': 65000})\ndict_salaries.update({'Alice': 55000})\nprint(dict_salaries) # can just write the object name\n\n{'Alice': 55000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 65000}\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nQuestion 2\nQ2(a). - Assign a variable Salary to 75000 - use an if-elif-else statement to print - Low if Salary is less than 50,000 - Medium if salary is betwen 50,000 and 99,999 - High if salary is 100,000 or more\n\nsalary = 75000\nif salary &lt; 50000:\n  print(\"Low\")\nelif 50000 &lt; salary &lt; 99999:\n  print(\"Medium\")\nelse:\n  print(\"High\")\n\nMedium\n\n\nQ2(b). - Assign two variables, role and salary to \"Manager\" and 85000, respectively. - use nested if-else statements to print - 'Eligible for bonus' if the role \"Manager\" is greater\n\nRole = 'Manager'\nSalary = 85000\nif Role == 'Manager' and Salary &gt; 80000:\n  print('Eligible for bonus')\nelif Role == 'Analyst' and Salary &lt; 60000:\n  print('Eligible for raise')\nelse:\n  print(\"No action needed\")\n\nEligible for bonus\n\n\n\n\nQuestion 3\nQ3(a). - Consider the following Salaries:\n\nlist_salaries = [45000, 60000, 75000, 120000, 30000]\n\n\nCalculate the average salary\nUse a for loop to print whether each salary is 'above average' or 'below average'.\n\n\nAverageSalary = sum(list_salaries) / len(list_salaries)\nfor GivenSalary in list_salaries:\n  if GivenSalary &gt; AverageSalary:\n    print(GivenSalary,' is Above Average')\n  elif GivenSalary &lt; AverageSalary:\n    print(\"Below Average\")\n\nBelow Average\nBelow Average\nAbove Average\nAbove Average\nBelow Average\n\n\nQ3(b). - Start with a salary of 50000. - Use a while loop to increase the salary by 5000 each year until it exceeds 80000 - Print the salary after each increment.\n\nsalary = 50000\nwhile salary &lt;= 80000:\n  salary += 5000 # Can use: salary = salary + 5000\n\nprint(salary)\n\n85000\n\n\nQ3(c). - Consider the following dictionary of employee salaries:\n\nsalaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 45000}\n\n\nUse a for loop to print the names of employees who earn more than 55000.\n\n\nfor name, salary in salaries.items(): #identifies both parts of the dictionary\n  if salary &gt; 55000:\n    print(name)\n\nBob\nCharlie\n\n\nQ3(d).\n\ndata_list = [42, 3.14, 'Data Analytics', True, None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\n\nGiven the list above, print the data type of each element using the type() function in a for loop. In the loop:\n\nConvert the integer 42 to a string.\nConvert the float 3.14 to a string, then back to a float.\nConvert the boolean True to an integer\n\n\n\nfor i in range(len(data_list)):\n    if type(data_list[i]) == int:\n        data_list[i] = str(data_list[i])\n    elif type(data_list[i]) == float:\n        data_list[i] = str(data_list[i])\n        data_list[i] = float(data_list[i])\n    elif type(data_list[i]) == bool:\n        data_list[i] = int(data_list[i])\n    print(type(data_list[i]))\ndata_list\n# I finally figured it out, it was just like the for loop in R studio, I just tried oversimplifying it initially\n\n&lt;class 'str'&gt;\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n&lt;class 'str'&gt;\n&lt;class 'NoneType'&gt;\n&lt;class 'list'&gt;\n&lt;class 'dict'&gt;\n&lt;class 'tuple'&gt;\n\n\n['42', 3.14, 'Data Analytics', '1', None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\n\n\n\nQuestion 4\nQ4(a). Consider the variables a and b:\n\na = 10\nb = 0\n\n\nUse a try-accept block to print the result of a / b.\n\nIf there is an error, print 'Cannot divide by zero!'.\n\n\n\ntry:\n  Solution = a / b\n  print(Solution)\nexcept:\n  print('Cannot divide by zero!')\n\nCannot divide by zero!\n\n\nQ4(b). - Consider the following dictionary of salaries with some missing (None) values:\n\nsalaries = {'Alice': 50000, 'Bob': None, 'Charlie': 70000, 'Dana': None, 'Eve': 80000}\n\n\nUse a for loop with a try-except block to calculate the total of non-missing salaries.\n\n\nSalarySum = [0]\nfor name, salary in salaries.items():\n  if salary != None:\n    SalarySum.append(salary)\nprint(\"The total of the Salaries:\",sum(SalarySum))\n# I don't entirely understand the try-except function nor when/how to implement\n\nThe total of the Salaries: 200000\n\n\n\n\nQuestion 5\n\nImport the math library and calculate the square root of 81 using the sqrt() function provided by the math library\n\n\nimport math # idk if this is a necessary line, but it won't let me unimport so I can't really check\nfrom math import sqrt\nsqrt(81)\n\n9.0"
  },
  {
    "objectID": "PortfolioDevelopment.html",
    "href": "PortfolioDevelopment.html",
    "title": "Portfolio Development",
    "section": "",
    "text": "NVDY Reinvestment Data Planner\nThe Dividend Reinvestment Calculator is an array of charted values based on a given amount of NVDY in terms of USD Valuation or number of shares.  \nThe given valuation will be more or less volatile, as the provided date comes from the date of dividend payouts, rather than all daily valuations. This is not meant to be a secured planner, but just a general visualization of how money invested into this can grow and by what growth the stock has already seen\nContributions\nSite Owner: Christopher Taratko\nContributors: Professor Li Lu & Professor Byeong-Hak Choe\n\n\n\n\n\n\n\nFunctionality of The Reinvestment Calculator:\nThe Calculator / Graph is designed to be a User Input based service (Which will occur in the future) that will be able to calculate the monthly and overall returns.\n\nThe current calculator does not have an input function due to the lack of JavaScript knowledge on my end, but it will be temporarily resolved with a ShinyApp variable slider (Ex. '$50,$100,$500,$1000,$5000... $100000' or in terms of # of shares)."
  },
  {
    "objectID": "PythonFolder/DANL210Project.html",
    "href": "PythonFolder/DANL210Project.html",
    "title": "DANL210Project",
    "section": "",
    "text": "The ESG is an important financial and technical set of values that relate to a company’s internal values, as well as their show of those internal values through values that make a difference in the world. ESG consists of three main portions: Environmental, Social, and Governance. To add onto this list of three, there’s the third value that is also looked at, which is Controversy.\n\n\nThe data collected was the 2025 stock data, as well as the ESG data of the past year, through Data Scraping. This method involves code to collect a company website’s data that they post, which turns it into then unreadable data, and then will convert the data back into readable data, such as a data frame, standardized text, or just understanding if a web-page has a button to click.\nimport pandas as pd\nurl_2024 = \"https://bcdanl.github.io/data/esg_proj_2024_data.csv\"\nesg_proj_2024_data = pd.read_csv(url_2024)\n\nurl_2025 = \"https://bcdanl.github.io/data/esg_proj_2025.csv\"\nesg_proj_2025 = pd.read_csv(url_2025)\n\ndel url_2024 # new favorite thing to use, clean explorer\ndel url_2025\n\nesg_proj_2025_filtered = esg_proj_2025[esg_proj_2025['Symbol'].isin(esg_proj_2024_data['Symbol'])].drop_duplicates().copy().reset_index(drop=True)\n# someone cooked here, and that person's name.... is me :sunglasses emoji:\ndel esg_proj_2025 #I'm keeping this explorer looking absotootly beautiful\nimport time\nimport random\nimport pandas as pd\nimport os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")  # Set window size\noptions.add_argument('--disable-blink-features=AutomationControlled')  # Prevent detection of automation by disabling blink features\noptions.page_load_strategy = 'eager'\ndriver = webdriver.Chrome(options=options)\n\ndf = pd.DataFrame()\nfor Ticker in esg_proj_2025_filtered['Symbol']:\n    url = f'https://finance.yahoo.com/quote/{Ticker}/sustainability/'\n    driver.get(url)\n    time.sleep(random.uniform(3,5))\n    Data = driver.find_elements(By.TAG_NAME,'h4')\n    try:\n        All_ESG = Data[0].text\n    except:\n        All_ESG = \"NA\"\n    try:\n        EnvRisk = Data[1].text\n    except:\n        EnvRisk = \"NA\"\n    try:\n        SocRisk = Data[2].text\n    except:\n        SocRisk = \"NA\"\n    try:\n        GovRisk = Data[3].text\n    except:\n        GovRisk = \"NA\"\n    try:\n        Controversy = driver.find_element(By.XPATH,\"/html/body/div[2]/main/section/section/section/article/section[3]/section[1]/div/div[2]/span[1]\").text\n    except:\n        Controversy = \"NA\"\n    \n    obs = [All_ESG, EnvRisk, SocRisk, GovRisk, Controversy]\n    obs = pd.DataFrame([obs])\n    df = pd.concat([df, obs], ignore_index= True)\ndriver.quit()\ndriver.close()\n\ndf"
  },
  {
    "objectID": "PythonFolder/DANL210Project.html#background-to-the-data",
    "href": "PythonFolder/DANL210Project.html#background-to-the-data",
    "title": "DANL210Project",
    "section": "",
    "text": "The data collected was the 2025 stock data, as well as the ESG data of the past year, through Data Scraping. This method involves code to collect a company website’s data that they post, which turns it into then unreadable data, and then will convert the data back into readable data, such as a data frame, standardized text, or just understanding if a web-page has a button to click.\nimport pandas as pd\nurl_2024 = \"https://bcdanl.github.io/data/esg_proj_2024_data.csv\"\nesg_proj_2024_data = pd.read_csv(url_2024)\n\nurl_2025 = \"https://bcdanl.github.io/data/esg_proj_2025.csv\"\nesg_proj_2025 = pd.read_csv(url_2025)\n\ndel url_2024 # new favorite thing to use, clean explorer\ndel url_2025\n\nesg_proj_2025_filtered = esg_proj_2025[esg_proj_2025['Symbol'].isin(esg_proj_2024_data['Symbol'])].drop_duplicates().copy().reset_index(drop=True)\n# someone cooked here, and that person's name.... is me :sunglasses emoji:\ndel esg_proj_2025 #I'm keeping this explorer looking absotootly beautiful\nimport time\nimport random\nimport pandas as pd\nimport os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")  # Set window size\noptions.add_argument('--disable-blink-features=AutomationControlled')  # Prevent detection of automation by disabling blink features\noptions.page_load_strategy = 'eager'\ndriver = webdriver.Chrome(options=options)\n\ndf = pd.DataFrame()\nfor Ticker in esg_proj_2025_filtered['Symbol']:\n    url = f'https://finance.yahoo.com/quote/{Ticker}/sustainability/'\n    driver.get(url)\n    time.sleep(random.uniform(3,5))\n    Data = driver.find_elements(By.TAG_NAME,'h4')\n    try:\n        All_ESG = Data[0].text\n    except:\n        All_ESG = \"NA\"\n    try:\n        EnvRisk = Data[1].text\n    except:\n        EnvRisk = \"NA\"\n    try:\n        SocRisk = Data[2].text\n    except:\n        SocRisk = \"NA\"\n    try:\n        GovRisk = Data[3].text\n    except:\n        GovRisk = \"NA\"\n    try:\n        Controversy = driver.find_element(By.XPATH,\"/html/body/div[2]/main/section/section/section/article/section[3]/section[1]/div/div[2]/span[1]\").text\n    except:\n        Controversy = \"NA\"\n    \n    obs = [All_ESG, EnvRisk, SocRisk, GovRisk, Controversy]\n    obs = pd.DataFrame([obs])\n    df = pd.concat([df, obs], ignore_index= True)\ndriver.quit()\ndriver.close()\n\ndf"
  }
]